{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import shutil\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot as plt \n",
    "from core.datahelper import DataSplitter, DataAE\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FcNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FcNet, self).__init__()\n",
    "\n",
    "        self.name = 'fc'\n",
    "\n",
    "        self.linear1 = nn.Linear(256 * 5 * 23, 300)\n",
    "        self.linear2 = nn.Linear(300, 100)\n",
    "        self.linear3 = nn.Linear(100, 50)\n",
    "        self.linear4 = nn.Linear(50, 20)\n",
    "        self.linear5 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        x = torch.relu(self.linear3(x))\n",
    "        x = torch.relu(self.linear4(x))\n",
    "        x = torch.sigmoid(self.linear5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.name = 'conv'\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm3 = nn.BatchNorm2d(32)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm4 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.linear5 = nn.Linear(4992, 100)\n",
    "        self.linear6 = nn.Linear(100, 50)\n",
    "        self.linear7 = nn.Linear(50, 20)\n",
    "        self.linear8 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.norm2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.norm3(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.norm4(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = torch.relu(self.linear5(x))\n",
    "        x = torch.relu(self.linear6(x))\n",
    "        x = torch.relu(self.linear7(x))\n",
    "        x = torch.sigmoid(self.linear8(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LstmNet, self).__init__()\n",
    "        self.name = 'lstm'\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm3 = nn.BatchNorm2d(32)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm4 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.lstm5 = torch.nn.LSTM(input_size=32, hidden_size=20, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.linear6 = nn.Linear(40, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.norm2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.norm3(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.norm4(x)\n",
    "\n",
    "        \n",
    "        x = torch.reshape(x, (-1, 32, 156))\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = x.contiguous()\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.lstm5(x)[0]\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        x = torch.sigmoid(self.linear6(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderNet, self).__init__()\n",
    "        self.name = 'encoder'\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm3 = nn.BatchNorm2d(32)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm4 = nn.BatchNorm2d(32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.norm2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.norm3(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.norm4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderNet, self).__init__()\n",
    "        self.name = 'decoder'\n",
    "\n",
    "        self.upconv1 = torch.nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        self.upconv2 = torch.nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        self.upconv3 = torch.nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=(4, 2))\n",
    "        self.norm3 = nn.BatchNorm2d(32)\n",
    "        self.upsample3 = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        self.upconv4 = torch.nn.ConvTranspose2d(in_channels=32, out_channels=1, kernel_size=(3, 2))\n",
    "        self.norm4 = nn.BatchNorm2d(1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.upconv1(x))\n",
    "\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        x = self.upsample1(x)\n",
    "\n",
    "        \n",
    "        x = torch.relu(self.upconv2(x))\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.upsample2(x)\n",
    "\n",
    "\n",
    "        x = torch.relu(self.upconv3(x))\n",
    "\n",
    "        x = self.norm3(x)\n",
    "        x = self.upsample3(x)\n",
    "\n",
    "\n",
    "        x = torch.relu(self.upconv4(x))\n",
    "\n",
    "        x = self.norm4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LockedDropout(nn.Module):\n",
    "    # ...\n",
    "    def forward(self, x, dropout=0.5):\n",
    "        if not self.training or not dropout:\n",
    "            return x\n",
    "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
    "        mask = torch.autograd.Variable(m, requires_grad=False) / (1 - dropout)\n",
    "        mask = mask.expand_as(x)\n",
    "        return mask * x\n",
    "\n",
    "class WeightDrop(torch.nn.Module):\n",
    "    def __init__(self, module, weights, dropout=0, variational=False):\n",
    "        super(WeightDrop, self).__init__()\n",
    "        self.module = module\n",
    "        self.weights = weights\n",
    "        self.dropout = dropout\n",
    "        self.variational = variational\n",
    "        self._setup()\n",
    "\n",
    "    def widget_demagnetizer_y2k_edition(*args, **kwargs):\n",
    "        # We need to replace flatten_parameters with a nothing function\n",
    "        # It must be a function rather than a lambda as otherwise pickling explodes\n",
    "        # We can't write boring code though, so ... WIDGET DEMAGNETIZER Y2K EDITION!\n",
    "        # (╯°□°）╯︵ ┻━┻\n",
    "        return\n",
    "\n",
    "    def _setup(self):\n",
    "        # Terrible temporary solution to an issue regarding compacting weights re: CUDNN RNN\n",
    "        if issubclass(type(self.module), torch.nn.RNNBase):\n",
    "            self.module.flatten_parameters = self.widget_demagnetizer_y2k_edition\n",
    "\n",
    "        for name_w in self.weights:\n",
    "            print('Applying weight drop of {} to {}'.format(self.dropout, name_w))\n",
    "            w = getattr(self.module, name_w)\n",
    "            del self.module._parameters[name_w]\n",
    "            self.module.register_parameter(name_w + '_raw', torch.nn.Parameter(w.data))\n",
    "\n",
    "    def _setweights(self):\n",
    "        for name_w in self.weights:\n",
    "            raw_w = getattr(self.module, name_w + '_raw')\n",
    "            w = None\n",
    "            if self.variational:\n",
    "                mask = torch.autograd.Variable(torch.ones(raw_w.size(0), 1))\n",
    "                if raw_w.is_cuda: mask = mask.cuda()\n",
    "                mask = torch.nn.functional.dropout(mask, p=self.dropout, training=True)\n",
    "                w = mask.expand_as(raw_w) * raw_w\n",
    "            else:\n",
    "                w = torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training)\n",
    "            if not self.training:\n",
    "                w = w.data\n",
    "            setattr(self.module, name_w, w)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        return self.module.forward(*args)\n",
    "\n",
    "def embedded_dropout(embed, words, dropout=0.1, scale=None):\n",
    "    if dropout:\n",
    "        mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\n",
    "        masked_embed_weight = mask * embed.weight\n",
    "    else:\n",
    "        masked_embed_weight = embed.weight\n",
    "    if scale:\n",
    "        masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n",
    "\n",
    "    padding_idx = embed.padding_idx\n",
    "    if padding_idx is None:\n",
    "        padding_idx = -1\n",
    "\n",
    "    X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
    "        padding_idx, embed.max_norm, embed.norm_type,\n",
    "        embed.scale_grad_by_freq, embed.sparse\n",
    "    )\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reccurent dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmDropNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LstmDropNet, self).__init__()\n",
    "        self.name = 'drop-lstm'\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm3 = nn.BatchNorm2d(32)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 2))\n",
    "        self.norm4 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.lstm5 = torch.nn.LSTM(input_size=32, hidden_size=20, bidirectional=True, batch_first=True)\n",
    "        self.lstm5 = WeightDrop(self.lstm5, ['weight_hh_l0', 'bias_hh_l0', 'weight_hh_l0_reverse', 'bias_hh_l0_reverse'], dropout=0.5) \n",
    "\n",
    "        self.linear6 = nn.Linear(40, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.norm2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.norm3(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.norm4(x)\n",
    "\n",
    "        \n",
    "        x = torch.reshape(x, (-1, 32, 156))\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = x.contiguous()\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.lstm5(x)[0]\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        x = torch.sigmoid(self.linear6(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLstmNet(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(EncoderLstmNet, self).__init__()\n",
    "        self.name = 'encoder-lstm'\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.encoder.eval()\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.lstm = torch.nn.LSTM(input_size=32, hidden_size=10, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        x = torch.reshape(x, (-1, 32, 156))\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = x.contiguous()\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.lstm(x)[0]\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        x = torch.sigmoid(self.linear(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        super().train()\n",
    "        self.encoder.eval()\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDropLstmNet(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(EncoderDropLstmNet, self).__init__()\n",
    "        self.name = 'encoder-drop-lstm'\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.encoder.eval()\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.lstm = torch.nn.LSTM(input_size=32, hidden_size=10, bidirectional=True, batch_first=True)\n",
    "        self.lstm = WeightDrop(self.lstm, ['weight_hh_l0', 'bias_hh_l0', 'weight_hh_l0_reverse', 'bias_hh_l0_reverse'], dropout=0.5)\n",
    "        self.linear = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        x = torch.reshape(x, (-1, 32, 156))\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = x.contiguous()\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.lstm(x)[0]\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        x = torch.sigmoid(self.linear(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        super().train()\n",
    "        self.encoder.eval()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyMetric:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = 'accuracy'\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def clean(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def accumulate(self, y_pred, y):\n",
    "        self.sum += (y_pred == y).sum().item() / len(y)\n",
    "        self.count += 1\n",
    "\n",
    "    def calculate(self):\n",
    "        return self.sum / self.count\n",
    "\n",
    "\n",
    "class RecallMetric:\n",
    "    def __init__(self):\n",
    "        self.name = 'recall'\n",
    "        self.tp = 0\n",
    "        self.fn = 0\n",
    "    \n",
    "    def clean(self):\n",
    "        self.tp = 0\n",
    "        self.fn = 0\n",
    "\n",
    "    def accumulate(self, y_pred, y):\n",
    "        self.tp += ((y_pred == y) & (y == 1)).sum().item()\n",
    "        self.fn += (y > y_pred).sum().item()\n",
    "\n",
    "    def calculate(self):\n",
    "        return self.tp / (self.tp + self.fn)\n",
    "\n",
    "\n",
    "class SpecifityMetric:\n",
    "    def __init__(self):\n",
    "        self.name = 'specifity'\n",
    "        self.tn = 0\n",
    "        self.n = 0\n",
    "    \n",
    "    def clean(self):\n",
    "        self.tn = 0\n",
    "        self.n = 0\n",
    "\n",
    "    def accumulate(self, y_pred, y):\n",
    "        self.tn += ((y_pred == y) & (y == 0)).sum().item()\n",
    "        self.n += (y == 0).sum().item()\n",
    "\n",
    "    def calculate(self):\n",
    "        return self.tn / self.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELDataset(Dataset):\n",
    "\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with record_function(\"get_data\"):\n",
    "            filepath, label = self.files[idx]\n",
    "            with record_function(\"np_read\"):\n",
    "                raw = np.load(filepath)\n",
    "            data = np.ascontiguousarray(raw.transpose(), dtype=np.float32).reshape(1, 1280, 23)\n",
    "            return data, np.array([label], dtype=np.float32)\n",
    "\n",
    "\n",
    "class NormilizeTransform(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return (sample - self.mean) / self.std\n",
    "\n",
    "class EEGRamDataset(Dataset):\n",
    "\n",
    "    def __init__(self, files, gen_normilize=False, transform=None):\n",
    "        self._data = np.zeros((len(files), 1, 1280, 23), dtype=np.float32)\n",
    "        self._labels = np.zeros((len(files), 1), dtype=np.float32)\n",
    "        for indx, (filepath, label) in enumerate(files):\n",
    "            raw = np.load(filepath)\n",
    "            data = np.ascontiguousarray(raw.transpose(), dtype=np.float32).reshape(1, 1280, 23)\n",
    "            self._data[indx] = data\n",
    "            self._labels[indx] = np.array([label], dtype=np.float32)\n",
    "\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "        if gen_normilize:\n",
    "            self.mean = self._data.mean()\n",
    "            self.std = self._data.std()\n",
    "\n",
    "            transform = NormilizeTransform(self.mean, self.std)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self._data[idx]\n",
    "        if self.transform:\n",
    "            return self.transform(data), self._labels[idx]\n",
    "        return data, self._labels[idx]\n",
    "\n",
    "    def get_mean_std(self):\n",
    "        return self.mean, self.std\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders_p_s(path_to_data, patient, seizure, mean=None, std=None, batch_size=64):\n",
    "    train, test = DataSplitter.load_files(path_to_data, path_to_data, patient, seizure)\n",
    "\n",
    "    if mean is not None:\n",
    "        train_data = EEGRamDataset(train, transform=NormilizeTransform(mean, std))\n",
    "    else:\n",
    "        train_data = EEGRamDataset(train, gen_normilize=True)\n",
    "        mean, std = train_data.get_mean_std()\n",
    "\n",
    "    # train_mean, train_std = train_data.get_mean_std()\n",
    "    test_data = EEGRamDataset(test, transform=NormilizeTransform(mean, std))\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "def get_ae_dataloaders(path_to_data, bacth_size):\n",
    "    train, test = DataAE.load_files(path_to_data, path_to_data)\n",
    "    train_dataset = EEGRamDataset(train, gen_normilize=True)\n",
    "    train_mean, train_std = train_dataset.get_mean_std()\n",
    "\n",
    "    train_data = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_data = DataLoader(EEGRamDataset(test, transform=NormilizeTransform(train_mean, train_std)), batch_size=bacth_size, shuffle=True)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientSpecTrainig:\n",
    "    mean = 0.21090616\n",
    "    std = 65.68308\n",
    "\n",
    "    def train_one_epoch(model, train_dataloader, loss_fn, optimizer, device='cpu'):\n",
    "        train_size = len(train_dataloader.dataset)\n",
    "        for i, (x, y) in enumerate(train_dataloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                loss, current = loss.item(), i * len(x)\n",
    "                print(f\"Batch Loss: {loss:>7f}  [{current:>5d}/{train_size:>5d}]\")\n",
    "\n",
    "    def train_patient_seizure(p_ind, s_indx, gen_model, train_dataloader, test_dataloader, path_to_models, loss_fn, optimizer_gen, metrics=[], steps=15, device='cpu', batch_size=32, rewrite=False):\n",
    "        model = gen_model().to(device)\n",
    "        optimizer = optimizer_gen(model)\n",
    "        # train_dataloader, test_dataloader = get_data_loaders_p_s(path_to_data, p_ind, s_indx, batch_size)\n",
    "        mean, std = train_dataloader.dataset.get_mean_std()\n",
    "        path_to_models = os.path.join(path_to_models, model.name, str(p_ind), str(s_indx))\n",
    "        epoch = 1\n",
    "\n",
    "        scheduler = StepLR(optimizer, step_size=300, gamma=0.5)\n",
    "\n",
    "        if rewrite:\n",
    "            if os.path.exists(path_to_models):\n",
    "                shutil.rmtree(path_to_models)\n",
    "            logs_path = os.path.join('logs', model.name, str(p_ind), str(s_indx))\n",
    "            if os.path.exists(logs_path):\n",
    "                shutil.rmtree(logs_path)\n",
    "        else:\n",
    "            if os.path.exists(path_to_models):\n",
    "                epochs = map(lambda x: int(x[:-3]), os.listdir(path_to_models))\n",
    "                if epochs == []:\n",
    "                    pass\n",
    "                else:\n",
    "                    load_epoch = max(epochs)\n",
    "                    checkpoint = torch.load(os.path.join(path_to_models, str(load_epoch) + '.pt'))\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                    # scheduler.load_state_dict(checkpoint['sheduler_state_dict'])\n",
    "                    epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "        \n",
    "        writer = SummaryWriter(os.path.join('logs', model.name, str(p_ind), str(s_indx)))\n",
    "\n",
    "        while epoch <= steps:\n",
    "            print('-'*30)\n",
    "            print('Epoch {}:'.format(epoch))\n",
    "            model.train(True)\n",
    "            PatientSpecTrainig.train_one_epoch(model, train_dataloader, loss_fn, optimizer, device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.train(False)\n",
    "                train_metrics = PatientSpecTrainig.get_metrics(model, train_dataloader, loss_fn, metrics, device)\n",
    "                val_metrics = PatientSpecTrainig.get_metrics(model, test_dataloader, loss_fn, metrics, device)\n",
    "\n",
    "                print('Train')\n",
    "                for key in train_metrics:\n",
    "                    print('{0}: {1}'.format(key, train_metrics[key]))\n",
    "                    writer.add_scalar(\"{0}/train\".format(key), train_metrics[key], epoch)\n",
    "\n",
    "\n",
    "                print('Test')\n",
    "                for key in val_metrics:\n",
    "                    print('{0}: {1}'.format(key, val_metrics[key]))\n",
    "                    writer.add_scalar(\"{0}/test\".format(key), val_metrics[key], epoch)\n",
    "\n",
    "                writer.flush()\n",
    "\n",
    "            scheduler.step()\n",
    "            PatientSpecTrainig.save_model(epoch, model, os.path.join(path_to_models, str(epoch) + '.pt'), optimizer, scheduler, mean, std, val_metrics)\n",
    "            epoch += 1\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "    def train_patient_model(p_ind, model_class, data_path, model_path, learning_parametrs, device='cpu', rewrite=False):\n",
    "        loss_fn = torch.nn.BCELoss().to(device)\n",
    "        gen_model = PatientSpecTrainig.get_model(model_class, path_to_encoder=os.path.join(path_to_models, 'autoencoder', '55.tar'))\n",
    "        for s_indx in range(DataSplitter.get_seizures_number(data_path, p_ind)):\n",
    "            print(\"Patient {0} - seizure {1}\".format(p_ind, s_indx))\n",
    "            mean = None\n",
    "            std = None\n",
    "            if model_class == EncoderLstmNet or model_class == EncoderDropLstmNet:\n",
    "                mean = PatientSpecTrainig.mean\n",
    "                std = PatientSpecTrainig.std\n",
    "            train_dataloader, test_dataloader = get_data_loaders_p_s(path_to_data, p_ind, s_indx, mean, std, 64)\n",
    "            PatientSpecTrainig.train_patient_seizure(p_ind, s_indx, gen_model, train_dataloader, test_dataloader, model_path,\n",
    "                                                     loss_fn, PatientSpecTrainig.get_optimizer, steps=learning_parametrs[model_class.__name__]['steps'],\n",
    "                                                     device=device, rewrite=rewrite, metrics=[AccuracyMetric(), RecallMetric(), SpecifityMetric()])\n",
    "    \n",
    "    def get_optimizer(model):\n",
    "        return torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
    "\n",
    "    def get_metrics(model, dataloader, loss_fn, metrics=[], device='cpu'):\n",
    "        validation_loss = 0.0\n",
    "        for m in metrics:\n",
    "            m.clean()\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred_v = model(x)\n",
    "            pred_round_v = pred_v.round()\n",
    "            loss_bacth = loss_fn(pred_v, y)\n",
    "            validation_loss += loss_bacth\n",
    "\n",
    "            for m in metrics:\n",
    "                m.accumulate(pred_round_v, y)\n",
    "\n",
    "        res = {'loss': validation_loss.item() / len(dataloader)}\n",
    "        for m in metrics:\n",
    "            res[m.name] = m.calculate()\n",
    "        return res\n",
    "    \n",
    "    def get_model(model_class, path_to_encoder=None):\n",
    "        if model_class == EncoderLstmNet or model_class == EncoderDropLstmNet:\n",
    "            def gen_model():\n",
    "                encoder = EncoderNet()\n",
    "                checkpoint_autoencoder = torch.load(path_to_encoder)\n",
    "                encoder.load_state_dict(checkpoint_autoencoder['encoder_state_dict'])\n",
    "                return model_class(encoder)\n",
    "            return gen_model\n",
    "        def gen_model():\n",
    "            return model_class()\n",
    "        return gen_model\n",
    "\n",
    "    def save_model(epoch, model, path_to_save, optimizer, sheduler, mean, std, metrics):\n",
    "        os.makedirs(os.path.dirname(path_to_save), exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': sheduler.state_dict(),\n",
    "            'mean': mean,\n",
    "            'std': std,\n",
    "            'metrics': metrics,\n",
    "            }, path_to_save)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"C:\\\\data\\\\CHBData\\\\preprocess\"\n",
    "path_to_models = 'D:\\CHBModel'\n",
    "patients_ids = [1, 3, 7, 9, 10, 20, 21, 22]\n",
    "learning_parametrs = {\n",
    "    'FcNet': {'steps': 50},\n",
    "    'ConvNet': {'steps': 50},\n",
    "    'LstmNet': {'steps': 50},\n",
    "    'EncoderLstmNet': {'steps': 300},\n",
    "    'LstmDropNet': {'steps': 100},\n",
    "    'EncoderDropLstmNet': {'steps': 300},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in patients_ids:\n",
    "    PatientSpecTrainig.train_patient_model(p, EncoderDropLstmNet, path_to_data, path_to_models, learning_parametrs, device='cuda', rewrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AETrainig:\n",
    "    def train_one_epoch(encoder, decoder, dataloader, loss_fn, encoder_optimizer, decoder_optimizer, device='cpu'):\n",
    "        train_size = len(dataloader.dataset)\n",
    "        for i, (x, _) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            \n",
    "            encoded = encoder(x)\n",
    "            decoded = decoder(encoded)\n",
    "\n",
    "            loss = loss_fn(decoded, x)\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            if i % 200 == 0:\n",
    "                loss, current = loss.item(), i * len(x)\n",
    "                print(f\"Batch Loss: {loss:>7f}  [{current:>5d}/{train_size:>5d}]\")\n",
    "\n",
    "\n",
    "\n",
    "    def train(train_dataloader, test_dataloader, path_to_models, device='cpu', steps=15, rewrite=False):\n",
    "        encoder = EncoderNet().to(device)\n",
    "        decoder = DecoderNet().to(device)\n",
    "        loss_fn = torch.nn.MSELoss().to(device)\n",
    "\n",
    "        encoder_optimizer = torch.optim.RMSprop(encoder.parameters(), lr=0.0002, alpha=0.98)\n",
    "        decoder_optimizer = torch.optim.RMSprop(decoder.parameters(), lr=0.0002, alpha=0.98)\n",
    "        \n",
    "        train_mean, train_std = train_dataloader.dataset.get_mean_std()\n",
    "\n",
    "        epoch = 1\n",
    "        logs_path = os.path.join('logs', 'autoencoder')\n",
    "        path_to_autoencoder = os.path.join(path_to_models, 'autoencoder')\n",
    "\n",
    "        if rewrite:\n",
    "            if os.path.exists(path_to_autoencoder):\n",
    "                shutil.rmtree(path_to_autoencoder)\n",
    "\n",
    "            if os.path.exists(logs_path):\n",
    "                shutil.rmtree(logs_path)\n",
    "        else:\n",
    "            epochs = map(lambda x: int(x[:-4]), os.listdir(path_to_autoencoder))\n",
    "            if epochs != []:\n",
    "                load_epoch = max(epochs)\n",
    "                checkpoint_autoencoder = torch.load(os.path.join(path_to_autoencoder, str(load_epoch) + '.tar'))\n",
    "                encoder.load_state_dict(checkpoint_autoencoder['encoder_state_dict'])\n",
    "                decoder.load_state_dict(checkpoint_autoencoder['decoder_state_dict'])\n",
    "                # encoder_optimizer.load_state_dict(checkpoint_autoencoder['encoder_optimizer_state_dict'])\n",
    "                # decoder_optimizer.load_state_dict(checkpoint_autoencoder['decoder_optimizer_state_dict'])\n",
    "                epoch = checkpoint_autoencoder['epoch'] + 1\n",
    "\n",
    "        writer = SummaryWriter(logs_path)\n",
    "\n",
    "        while epoch <= steps:\n",
    "            print('-'*30)\n",
    "            print('Epoch {}:'.format(epoch))\n",
    "\n",
    "            encoder.train(True)\n",
    "            decoder.train(True)\n",
    "\n",
    "            AETrainig.train_one_epoch_ae(encoder, decoder, train_dataloader, loss_fn, encoder_optimizer, decoder_optimizer, device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                encoder.train(False)\n",
    "                decoder.train(False)\n",
    "\n",
    "                train_loss = 0\n",
    "                for x, _ in train_dataloader:\n",
    "                    x = x.to(device)\n",
    "                    train_loss += loss_fn(decoder(encoder(x)), x)\n",
    "                train_loss /= len(train_dataloader)\n",
    "\n",
    "                test_loss = 0\n",
    "                for x, _ in test_dataloader:\n",
    "                    x = x.to(device)\n",
    "                    test_loss += loss_fn(decoder(encoder(x)), x)\n",
    "                test_loss /= len(test_dataloader)\n",
    "\n",
    "                print('Train')\n",
    "                print('{0}: {1}'.format('Loss ', train_loss))\n",
    "                writer.add_scalar(\"{0}/train\".format('Loss'), train_loss, epoch)\n",
    "\n",
    "\n",
    "                print('Test')\n",
    "                print('{0}: {1}'.format('Loss ', test_loss))\n",
    "                writer.add_scalar(\"{0}/test\".format('Loss'), test_loss, epoch)\n",
    "\n",
    "                writer.flush()\n",
    "\n",
    "                AETrainig.save_autoencoder(os.path.join(path_to_autoencoder, str(epoch) + '.tar'), epoch, encoder, decoder, encoder_optimizer, decoder_optimizer, train_mean, train_std, None)\n",
    "            epoch += 1\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "    def save(path_to_save, epoch, encoder, decoder, encoder_optimizer, decoder_optimizer, mean, std, metrics):\n",
    "        os.makedirs(os.path.dirname(path_to_save), exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'decoder_state_dict': decoder.state_dict(),\n",
    "            'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "            'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "            'mean': mean,\n",
    "            'std': std,\n",
    "            'metrics': metrics,\n",
    "            }, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_train_dataloader, ae_test_dataloader = get_ae_dataloaders(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AETrainig.train(ae_train_dataloader, ae_test_dataloader, path_to_models, device='cuda', steps=100, rewrite=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb1dc47a6e91b21b0f67003614e44b2cffacf00b440b4376a4c574844d3a39a3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
